{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_intro_imports",
   "metadata": {},
   "source": [
    "# Import Dependencies\n",
    "- Import common libraries for data analysis, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12fcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7deac91",
   "metadata": {},
   "source": [
    "# Data Loading and Preview\n",
    "- Load `sf-crime/train.csv`\n",
    "- Preview the first rows and summarize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3adcdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows:\n",
      "                 Dates        Category                      Descript  \\\n",
      "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
      "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "\n",
      "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
      "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
      "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
      "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.425892  37.774599  \n",
      "1 -122.425892  37.774599  \n",
      "2 -122.424363  37.800414  \n",
      "3 -122.426995  37.800873  \n",
      "4 -122.438738  37.771541  \n",
      "Missing values:\n",
      "Dates         0\n",
      "Category      0\n",
      "Descript      0\n",
      "DayOfWeek     0\n",
      "PdDistrict    0\n",
      "Resolution    0\n",
      "Address       0\n",
      "X             0\n",
      "Y             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('sf-crime/train.csv')\n",
    "\n",
    "# A. Data Understanding and Preprocessing\n",
    "\n",
    "# 1. Check the first few rows of the dataset\n",
    "print('First few rows:')\n",
    "print(df.head())\n",
    "\n",
    "# 2. Check for missing values\n",
    "print('Missing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cleaning",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- Parse and clean the date column (drop unparseable records)\n",
    "- Remove duplicate rows\n",
    "- Filter out invalid coordinates (keep within San Francisco bounds)\n",
    "- Print dataset shape after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975aa7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (878049, 9)\n",
      "Dates          object\n",
      "Category       object\n",
      "Descript       object\n",
      "DayOfWeek      object\n",
      "PdDistrict     object\n",
      "Resolution     object\n",
      "Address        object\n",
      "X             float64\n",
      "Y             float64\n",
      "dtype: object\n",
      "Duplicate rows: 2323\n",
      "Removed 67 rows outside SF bounds\n",
      "Cleaned shape: (875659, 9)\n"
     ]
    }
   ],
   "source": [
    "# 3. Basic structure and types\n",
    "print('Shape:', df.shape)\n",
    "print(df.dtypes)\n",
    "\n",
    "# 4. Parse date column and handle invalid values\n",
    "date_col = 'Dates' if 'Dates' in df.columns else None\n",
    "if date_col:\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    invalid_dates = df[date_col].isna().sum()\n",
    "    if invalid_dates:\n",
    "        print(f'Invalid dates: {invalid_dates} \u2014 dropping them')\n",
    "        df = df[df[date_col].notna()].copy()\n",
    "else:\n",
    "    print('No date column found')\n",
    "\n",
    "# 5. Drop exact duplicates\n",
    "dup_count = df.duplicated().sum()\n",
    "print('Duplicate rows:', dup_count)\n",
    "if dup_count:\n",
    "    df = df.drop_duplicates().copy()\n",
    "\n",
    "# 6. Sanity check coordinates (keep plausible SF bounds)\n",
    "if {'X','Y'}.issubset(df.columns):\n",
    "    before = len(df)\n",
    "    df = df[df['Y'].between(37.0, 38.0) & df['X'].between(-123.0, -121.0)].copy()\n",
    "    print(f'Removed {before - len(df)} rows outside SF bounds')\n",
    "\n",
    "print('Cleaned shape:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_features",
   "metadata": {},
   "source": [
    "# Feature Engineering and Encoding\n",
    "- Construct Arrest_Indicator, Year/Month/Day/Hour, Incident_Quarter\n",
    "- One-hot encode DayOfWeek, PdDistrict, Resolution; label-encode Category if present\n",
    "- Example filters: create subsets by year (e.g., 2015) and district (e.g., Mission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d025fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: (875659, 44)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Arrest_Indicator</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>...</th>\n",
       "      <th>Resolution_JUVENILE CITED</th>\n",
       "      <th>Resolution_JUVENILE DIVERTED</th>\n",
       "      <th>Resolution_LOCATED</th>\n",
       "      <th>Resolution_NONE</th>\n",
       "      <th>Resolution_NOT PROSECUTED</th>\n",
       "      <th>Resolution_PROSECUTED BY OUTSIDE AGENCY</th>\n",
       "      <th>Resolution_PROSECUTED FOR LESSER OFFENSE</th>\n",
       "      <th>Resolution_PSYCHOPATHIC CASE</th>\n",
       "      <th>Resolution_UNFOUNDED</th>\n",
       "      <th>Category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                      Descript  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "3 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "4 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "\n",
       "                     Address           X          Y  Arrest_Indicator  Year  \\\n",
       "0         OAK ST / LAGUNA ST -122.425892  37.774599                 1  2015   \n",
       "1         OAK ST / LAGUNA ST -122.425892  37.774599                 1  2015   \n",
       "2  VANNESS AV / GREENWICH ST -122.424363  37.800414                 1  2015   \n",
       "3   1500 Block of LOMBARD ST -122.426995  37.800873                 0  2015   \n",
       "4  100 Block of BRODERICK ST -122.438738  37.771541                 0  2015   \n",
       "\n",
       "   Month  Day  ...  Resolution_JUVENILE CITED Resolution_JUVENILE DIVERTED  \\\n",
       "0      5   13  ...                      False                        False   \n",
       "1      5   13  ...                      False                        False   \n",
       "2      5   13  ...                      False                        False   \n",
       "3      5   13  ...                      False                        False   \n",
       "4      5   13  ...                      False                        False   \n",
       "\n",
       "   Resolution_LOCATED  Resolution_NONE  Resolution_NOT PROSECUTED  \\\n",
       "0               False            False                      False   \n",
       "1               False            False                      False   \n",
       "2               False            False                      False   \n",
       "3               False             True                      False   \n",
       "4               False             True                      False   \n",
       "\n",
       "   Resolution_PROSECUTED BY OUTSIDE AGENCY  \\\n",
       "0                                    False   \n",
       "1                                    False   \n",
       "2                                    False   \n",
       "3                                    False   \n",
       "4                                    False   \n",
       "\n",
       "   Resolution_PROSECUTED FOR LESSER OFFENSE  Resolution_PSYCHOPATHIC CASE  \\\n",
       "0                                     False                         False   \n",
       "1                                     False                         False   \n",
       "2                                     False                         False   \n",
       "3                                     False                         False   \n",
       "4                                     False                         False   \n",
       "\n",
       "   Resolution_UNFOUNDED  Category_encoded  \n",
       "0                 False                37  \n",
       "1                 False                21  \n",
       "2                 False                21  \n",
       "3                 False                16  \n",
       "4                 False                16  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in 2015: 27522\n",
      "Rows in Mission district: 119722\n"
     ]
    }
   ],
   "source": [
    "# 7. Derived variables: Arrest Indicator, Incident Quarter, calendar parts\n",
    "\n",
    "if 'Resolution' in df.columns:\n",
    "\n",
    "    df['Arrest_Indicator'] = df['Resolution'].str.contains('ARREST', case=False, na=False).astype(int)\n",
    "\n",
    "\n",
    "if 'Dates' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Dates']):\n",
    "\n",
    "    df['Year'] = df['Dates'].dt.year\n",
    "\n",
    "    df['Month'] = df['Dates'].dt.month\n",
    "\n",
    "    df['Day'] = df['Dates'].dt.day\n",
    "\n",
    "    df['Hour'] = df['Dates'].dt.hour\n",
    "\n",
    "    df['Incident_Quarter'] = df['Dates'].dt.to_period('Q').astype(str)\n",
    "\n",
    "\n",
    "# 8. Encode categorical variables (one-hot)\n",
    "\n",
    "categorical_cols = [c for c in ['DayOfWeek','PdDistrict','Resolution'] if c in df.columns]\n",
    "\n",
    "for c in categorical_cols:\n",
    "\n",
    "    df[c] = df[c].astype('category')\n",
    "\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "# Optionally encode target for modeling later\n",
    "\n",
    "if 'Category' in df_encoded.columns:\n",
    "\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    df_encoded['Category_encoded'] = le.fit_transform(df_encoded['Category'])\n",
    "\n",
    "\n",
    "print('Encoded shape:', df_encoded.shape)\n",
    "\n",
    "display(df_encoded.head())\n",
    "\n",
    "\n",
    "# 9. Optional filter examples (by year and district)\n",
    "\n",
    "if 'Year' in df_encoded.columns:\n",
    "\n",
    "    df_2015 = df_encoded[df_encoded['Year'] == 2015].copy()\n",
    "\n",
    "else:\n",
    "\n",
    "    df_2015 = df_encoded.copy()\n",
    "\n",
    "\n",
    "if 'PdDistrict' in df.columns:\n",
    "\n",
    "    mission_idx = df['PdDistrict'] == 'MISSION'\n",
    "\n",
    "    df_mission = df_encoded[mission_idx].copy()\n",
    "\n",
    "else:\n",
    "\n",
    "    df_mission = df_encoded.copy()\n",
    "\n",
    "\n",
    "print('Rows in 2015:', len(df_2015))\n",
    "\n",
    "print('Rows in Mission district:', len(df_mission))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cbfde7",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "- Select features and build train/test sets\n",
    "- Compare multiple algorithms (Logistic Regression, Decision Tree, Random Forest; include XGBoost if available)\n",
    "- Use cross-validation with Accuracy and Macro-F1\n",
    "- Report test Accuracy and Macro-F1, and plot the confusion matrix\n",
    "- Perform hyperparameter tuning (GridSearchCV)\n",
    "- Visualize important features and model structure/relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4483a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 21\n",
      "Sample features: ['X', 'Y', 'Year', 'Month', 'Day', 'Hour', 'DayOfWeek_Monday', 'DayOfWeek_Saturday', 'DayOfWeek_Sunday', 'DayOfWeek_Thursday']\n",
      "Train/Test shapes: (700527, 21) (175132, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10. Feature selection for modeling the Category\n",
    "# Target\n",
    "if 'Category_encoded' not in df_encoded.columns:\n",
    "    raise ValueError('Target `Category_encoded` not found; ensure earlier cell ran successfully.')\n",
    "\n",
    "y = df_encoded['Category_encoded']\n",
    "\n",
    "# We'll avoid leakage by excluding Resolution-derived columns and Arrest_Indicator.\n",
    "# Also avoid high-cardinality Address_* dummies in features to keep model tractable.\n",
    "# Keep time parts, coordinates, DayOfWeek_*, PdDistrict_*\n",
    "numeric_keep = [c for c in ['Year','Month','Day','Hour','X','Y'] if c in df_encoded.columns]\n",
    "keep_prefixes = ('DayOfWeek_', 'PdDistrict_')\n",
    "\n",
    "X_cols = []\n",
    "for c in df_encoded.columns:\n",
    "    if c in numeric_keep:\n",
    "        X_cols.append(c)\n",
    "    elif c.startswith(keep_prefixes):\n",
    "        X_cols.append(c)\n",
    "\n",
    "# Sanity: Ensure we have features\n",
    "if not X_cols:\n",
    "    raise ValueError('No features selected. Check earlier encoding steps.')\n",
    "\n",
    "X = df_encoded[X_cols].copy()\n",
    "print('Selected features:', len(X_cols))\n",
    "print('Sample features:', X_cols[:10])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print('Train/Test shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c1ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\ananconda\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:00:11] WARNING: D:\\bld\\xgboost-split_1733179550881\\work\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV (5-fold) results:\n",
      "Logistic     | Acc: 0.201 \u00b1 0.001 | F1_macro: 0.012 \u00b1 0.000\n",
      "DecisionTree | Acc: 0.237 \u00b1 0.000 | F1_macro: 0.112 \u00b1 0.002\n",
      "RandomForest | Acc: 0.304 \u00b1 0.000 | F1_macro: 0.101 \u00b1 0.001\n",
      "XGBoost      | Acc: 0.281 \u00b1 0.001 | F1_macro: 0.090 \u00b1 0.002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 11. Cross-validation comparison of baseline models\n",
    "\n",
    "models = {\n",
    "    'Logistic': LogisticRegression(max_iter=1000, n_jobs=1),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=1)\n",
    "}\n",
    "\n",
    "# Optionally include XGBoost if available\n",
    "try:\n",
    "    models['XGBoost'] = XGBClassifier(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "        objective='multi:softprob', eval_metric='mlogloss', tree_method='hist',device='cuda', n_jobs=1, random_state=42\n",
    "    )\n",
    "except Exception as e:\n",
    "    print('XGBoost not available, skipping.')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {'acc':'accuracy','f1_macro':'f1_macro','logloss':'neg_log_loss'}\n",
    "\n",
    "cv_results = {}\n",
    "for name, clf in models.items():\n",
    "    res = cross_validate(clf, X, y, cv=cv, scoring=scoring, n_jobs=1, return_train_score=False)\n",
    "    cv_results[name] = {k: (v.mean(), v.std()) for k, v in res.items() if k.startswith('test_')}\n",
    "\n",
    "print('CV (5-fold) results:')\n",
    "for name, res in cv_results.items():\n",
    "    logloss_mean = -res['test_logloss'][0]\n",
    "    logloss_std = res['test_logloss'][1]\n",
    "    print(f\"{name:12s} | Acc: {res['test_acc'][0]:.3f} \u00b1 {res['test_acc'][1]:.3f} | F1_macro: {res['test_f1_macro'][0]:.3f} \u00b1 {res['test_f1_macro'][1]:.3f} | LogLoss: {logloss_mean:.3f} \u00b1 {logloss_std:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc5869a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 5\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      6\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      7\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1291\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[0;32m   1292\u001b[0m     path_func(\n\u001b[0;32m   1293\u001b[0m         X,\n\u001b[0;32m   1294\u001b[0m         y,\n\u001b[0;32m   1295\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[0;32m   1296\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[0;32m   1297\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[0;32m   1298\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1299\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1300\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1301\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[0;32m   1302\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[0;32m   1303\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1304\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1305\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1306\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1307\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[0;32m   1308\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[0;32m   1309\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[0;32m   1310\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1311\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m   1312\u001b[0m     )\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[0;32m   1314\u001b[0m )\n\u001b[0;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    446\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[0;32m    447\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    448\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    449\u001b[0m ]\n\u001b[1;32m--> 450\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    451\u001b[0m     func,\n\u001b[0;32m    452\u001b[0m     w0,\n\u001b[0;32m    453\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    454\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    455\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    456\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: iprint, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: tol, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_iter},\n\u001b[0;32m    457\u001b[0m )\n\u001b[0;32m    458\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    459\u001b[0m     solver,\n\u001b[0;32m    460\u001b[0m     opt_res,\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    464\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:278\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[1;32m--> 278\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_loss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    279\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    280\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    281\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    282\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    285\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n",
      "File \u001b[1;32me:\\ananconda\\Lib\\site-packages\\sklearn\\_loss\\loss.py:257\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[1;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m ReadonlyArrayWrapper(sample_weight)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcloss\u001b[38;5;241m.\u001b[39mloss_gradient(\n\u001b[0;32m    258\u001b[0m     y_true\u001b[38;5;241m=\u001b[39my_true,\n\u001b[0;32m    259\u001b[0m     raw_prediction\u001b[38;5;241m=\u001b[39mraw_prediction,\n\u001b[0;32m    260\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    261\u001b[0m     loss_out\u001b[38;5;241m=\u001b[39mloss_out,\n\u001b[0;32m    262\u001b[0m     gradient_out\u001b[38;5;241m=\u001b[39mgradient_out,\n\u001b[0;32m    263\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    264\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 12. Train/test evaluation and confusion matrix\n",
    "\n",
    "fit_results = {}\n",
    "for name, clf in models.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average='macro')\n",
    "    ll = log_loss(y_test, y_proba, labels=clf.classes_)\n",
    "    fit_results[name] = {'acc': acc, 'f1_macro': f1m, 'logloss': ll, 'y_pred': y_pred, 'y_proba': y_proba}\n",
    "    print(f\"{name} -> Test Accuracy: {acc:.3f}, Macro-F1: {f1m:.3f}, LogLoss: {ll:.3f}\")\n",
    "\n",
    "# Pick best by Macro-F1\n",
    "best_name = max(fit_results, key=lambda k: fit_results[k]['f1_macro'])\n",
    "print(f\"Best (by Macro-F1): {best_name}\")\n",
    "\n",
    "y_pred_best = fit_results[best_name]['y_pred']\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix \u2014 {best_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Classification report for best model:')\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115105d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 13. Hyperparameter tuning (GridSearchCV)\n",
    "\n",
    "search_spaces = {\n",
    "    'Logistic': (LogisticRegression(max_iter=1000),\n",
    "                 {'C':[0.1,1,3], 'penalty':['l2'], 'solver':['lbfgs'] }),\n",
    "    'DecisionTree': (DecisionTreeClassifier(random_state=42),\n",
    "                     {'max_depth':[None,10,20], 'min_samples_split':[2,10,50]}),\n",
    "    'RandomForest': (RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                     {'n_estimators':[200,400], 'max_depth':[None,20], 'min_samples_split':[2,10]})\n",
    "}\n",
    "\n",
    "best_estimators = {}\n",
    "for name, (est, grid) in search_spaces.items():\n",
    "    print(f\"Tuning {name} ...\")\n",
    "    gs = GridSearchCV(est, grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    print('Best params:', gs.best_params_)\n",
    "    print('Best CV f1_macro:', gs.best_score_)\n",
    "    best_estimators[name] = gs.best_estimator_\n",
    "\n",
    "# Evaluate tuned models\n",
    "for name, est in best_estimators.items():\n",
    "    y_pred = est.predict(X_test)\n",
    "    y_proba = est.predict_proba(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average='macro')\n",
    "    ll = log_loss(y_test, y_proba, labels=est.classes_)\n",
    "    print(f\"{name} (tuned) -> Test Accuracy: {acc:.3f}, Macro-F1: {f1m:.3f}, LogLoss: {ll:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 14. Visualizations: feature importance and tree structure (if applicable)\n",
    "\n",
    "# Feature importances for RandomForest (tuned if available)\n",
    "rf = best_estimators.get('RandomForest', models.get('RandomForest'))\n",
    "if hasattr(rf, 'feature_importances_'):\n",
    "    importances = rf.feature_importances_\n",
    "    idx = np.argsort(importances)[::-1][:15]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.barh(range(len(idx)), importances[idx][::-1])\n",
    "    plt.yticks(range(len(idx)), [X.columns[i] for i in idx][::-1])\n",
    "    plt.title('Top 15 Feature Importances \u2014 RandomForest')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot a shallow Decision Tree for interpretability\n",
    "clf_tree = best_estimators.get('DecisionTree', DecisionTreeClassifier(max_depth=3, random_state=42))\n",
    "if not hasattr(clf_tree, 'tree_'):\n",
    "    clf_tree.fit(X_train, y_train)\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(clf_tree, feature_names=X.columns.tolist(), max_depth=3, filled=True, fontsize=6)\n",
    "plt.title('Decision Tree (depth<=3)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and Insights\n",
    "- Highlight feature influence on predicted crime type\n",
    "- Explore bias motives (if present/derivable) linked to offense categories\n",
    "- Look at temporal patterns to inform proactive interventions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Which features most influence predicted crime type?\n",
    "rf_fit = best_estimators.get('RandomForest', models.get('RandomForest'))\n",
    "log_fit = best_estimators.get('Logistic', models.get('Logistic'))\n",
    "\n",
    "if hasattr(rf_fit, 'feature_importances_'):\n",
    "    fi = pd.Series(rf_fit.feature_importances_, index=X.columns)\n",
    "    top_rf = fi.sort_values(ascending=False).head(12)\n",
    "    print('Top features by RandomForest importance:')\n",
    "    display(top_rf.to_frame('importance'))\n",
    "else:\n",
    "    top_rf = pd.Series(dtype=float)\n",
    "    print('RandomForest importances not available.')\n",
    "\n",
    "if hasattr(log_fit, 'coef_') and log_fit.coef_.size:\n",
    "    coef_strength = pd.Series(np.abs(log_fit.coef_).mean(axis=0), index=X.columns)\n",
    "    top_log = coef_strength.sort_values(ascending=False).head(12)\n",
    "    print('\\nMost influential logistic regression features (avg |coef| across classes):')\n",
    "    display(top_log.to_frame('avg_abs_coef'))\n",
    "else:\n",
    "    try:\n",
    "        log_fit = log_fit.fit(X_train, y_train)\n",
    "        coef_strength = pd.Series(np.abs(log_fit.coef_).mean(axis=0), index=X.columns)\n",
    "        top_log = coef_strength.sort_values(ascending=False).head(12)\n",
    "        print('\\nMost influential logistic regression features (avg |coef| across classes):')\n",
    "        display(top_log.to_frame('avg_abs_coef'))\n",
    "    except Exception as e:\n",
    "        top_log = pd.Series(dtype=float)\n",
    "        print('Logistic coefficients unavailable:', e)\n",
    "\n",
    "# Consolidated view for quick briefing\n",
    "if not top_rf.empty or not top_log.empty:\n",
    "    influence_summary = pd.concat({\n",
    "        'RandomForest': top_rf,\n",
    "        'Logistic_avg_abs_coef': top_log\n",
    "    }, axis=1)\n",
    "    display(influence_summary)\n",
    "else:\n",
    "    print('No feature influence metrics to summarize.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Bias motive (if available) vs offense category\n",
    "import re\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "bias_candidates = [c for c in df.columns if 'bias' in c.lower()]\n",
    "df_bias = None\n",
    "\n",
    "if bias_candidates:\n",
    "    bias_col = bias_candidates[0]\n",
    "    df_bias = df.copy()\n",
    "    df_bias['BiasMotive'] = df[bias_col].fillna('Unspecified')\n",
    "    print(f'Bias motive source: {bias_col}')\n",
    "elif 'Descript' in df.columns:\n",
    "    bias_keywords = {\n",
    "        'Racial/Ethnic': ['RACIAL', 'RACIST', 'BLACK', 'AFRICAN', 'ASIAN', 'CHINESE', 'KOREAN', 'VIETNAMESE', 'LATINO', 'HISPANIC', 'MEXICAN'],\n",
    "        'Religious': ['JEW', 'MUSLIM', 'ISLAM', 'CHRISTIAN', 'CATHOLIC', 'BUDDHIST'],\n",
    "        'Gender/Sexuality': ['GAY', 'LGBT', 'LESBIAN', 'HOMOPHOB', 'TRANS'],\n",
    "        'Disability': ['DISABILITY', 'DISABLED']\n",
    "    }\n",
    "\n",
    "    def infer_bias(text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return 'Unspecified'\n",
    "        t = str(text).upper()\n",
    "        for motive, keywords in bias_keywords.items():\n",
    "            if any(k in t for k in keywords):\n",
    "                return motive\n",
    "        return 'Unspecified'\n",
    "\n",
    "    df_bias = df.copy()\n",
    "    df_bias['BiasMotive'] = df_bias['Descript'].apply(infer_bias)\n",
    "    print('Bias motive derived from `Descript` keywords (proxy).')\n",
    "else:\n",
    "    print('No descriptive text available to infer bias motives; skipping cross-analysis.')\n",
    "\n",
    "if df_bias is not None:\n",
    "    bias_counts = df_bias['BiasMotive'].value_counts()\n",
    "    print('\\nBias motive counts (top 10):')\n",
    "    display(bias_counts.head(10).to_frame('count'))\n",
    "\n",
    "    filtered_bias = df_bias[df_bias['BiasMotive'] != 'Unspecified']\n",
    "    if filtered_bias.empty:\n",
    "        print('No explicit/inferred bias motives found \u2014 typical for general SF crime data.')\n",
    "    else:\n",
    "        top_categories = df['Category'].value_counts().head(8).index if 'Category' in df.columns else None\n",
    "        cross = pd.crosstab(filtered_bias['BiasMotive'], filtered_bias['Category'])\n",
    "        if top_categories is not None:\n",
    "            cross = cross.loc[:, [c for c in top_categories if c in cross.columns]]\n",
    "        if cross.empty:\n",
    "            print('No overlap between bias motives and categories after filtering.')\n",
    "        else:\n",
    "            cross_pct = cross.div(cross.sum(axis=1), axis=0)\n",
    "            print('\\nBias motive -> offense category (row-normalized):')\n",
    "            display(cross_pct)\n",
    "\n",
    "            chi2, p, dof, exp = chi2_contingency(cross)\n",
    "            cramers_v = np.sqrt(chi2 / (cross.values.sum() * (min(cross.shape) - 1))) if min(cross.shape) > 1 else np.nan\n",
    "            print(f'Cramers V association: {cramers_v:.3f} (p={p:.3e})')\n",
    "\n",
    "            top_pairs = cross_pct.stack().sort_values(ascending=False).head(8)\n",
    "            print('\\nTop Bias motive / Category combinations:')\n",
    "            display(top_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Temporal trends (monthly/quarterly) for categories and bias motives\n",
    "if 'Dates' in df.columns and pd.api.types.is_datetime64_any_dtype(df['Dates']):\n",
    "    df_time = df.copy()\n",
    "    df_time['MonthStart'] = df_time['Dates'].dt.to_period('M').dt.to_timestamp()\n",
    "    top_cats = df_time['Category'].value_counts().head(5).index if 'Category' in df_time.columns else []\n",
    "    if len(top_cats):\n",
    "        monthly_cat = df_time[df_time['Category'].isin(top_cats)].groupby(['MonthStart','Category']).size().reset_index(name='count')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.lineplot(data=monthly_cat, x='MonthStart', y='count', hue='Category')\n",
    "        plt.title('Monthly counts for top crime categories')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Incidents')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Category column missing; skipping category trend lines.')\n",
    "\n",
    "    if df_bias is not None:\n",
    "        df_time_bias = df_bias.copy()\n",
    "        df_time_bias = df_time_bias[df_time_bias['BiasMotive'] != 'Unspecified']\n",
    "        if not df_time_bias.empty:\n",
    "            df_time_bias['MonthStart'] = df_time_bias['Dates'].dt.to_period('M').dt.to_timestamp()\n",
    "            monthly_bias = df_time_bias.groupby(['MonthStart','BiasMotive']).size().reset_index(name='count')\n",
    "            plt.figure(figsize=(10,4))\n",
    "            sns.lineplot(data=monthly_bias, x='MonthStart', y='count', hue='BiasMotive')\n",
    "            plt.title('Monthly counts of inferred bias-motivated incidents')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Incidents')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('No inferred bias-motivated incidents to plot.')\n",
    "else:\n",
    "    print('Date column missing or not parsed; temporal analysis skipped.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Actionable recommendations (data-driven bullets)\n",
    "recommendations = []\n",
    "\n",
    "if 'PdDistrict' in df.columns and len(df):\n",
    "    district_counts = df['PdDistrict'].value_counts().head(3)\n",
    "    share = district_counts.sum() / len(df)\n",
    "    recommendations.append(f\"Concentrate multi-lingual awareness campaigns and foot patrols in {', '.join(district_counts.index)} (cover {share:.1%} of incidents).\")\n",
    "\n",
    "if 'Hour' in df.columns:\n",
    "    peak_hours = df['Hour'].value_counts().nlargest(3).index.tolist()\n",
    "    recommendations.append(f\"Schedule community education and visible policing around peak hours {peak_hours} to deter high-volume incidents.\")\n",
    "\n",
    "if df_bias is not None and 'BiasMotive' in df_bias.columns:\n",
    "    top_bias = df_bias[df_bias['BiasMotive'] != 'Unspecified']['BiasMotive'].value_counts().head(3)\n",
    "    if not top_bias.empty:\n",
    "        bias_targets = ', '.join(top_bias.index)\n",
    "        recommendations.append(f\"Co-design outreach with community leaders to address bias motives: {bias_targets}.\")\n",
    "    else:\n",
    "        recommendations.append('No explicit bias motives detected; prioritize improving data capture on bias indicators in incident reports.')\n",
    "else:\n",
    "    recommendations.append('Bias motive data absent; recommend adding fields/training for bias identification to support targeted interventions.')\n",
    "\n",
    "if 'Category' in df.columns and not df['Category'].empty:\n",
    "    top_cat = df['Category'].value_counts().idxmax()\n",
    "    recommendations.append(f\"Allocate investigative resources to the leading offense category ({top_cat}) and consider dedicated task-force time blocks.\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print('-', rec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}